{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoogLeNet_chest_X_ray_image_binary_classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMufrL6kIsbwBkxshbAuSrl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyesukim1/chest_X_ray_images_binary_classification/blob/main/GoogLeNet_chest_X_ray_image_binary_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle API로 연결하여 데이터 로드\n"
      ],
      "metadata": {
        "id": "puLQWAzJlFUj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWTESRSCk1XV"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia"
      ],
      "metadata": {
        "id": "npG138wllaj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "5R6jEk2wlpF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq \"/content/chest-xray-pneumonia.zip\""
      ],
      "metadata": {
        "id": "RybT-WpUl6Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "WvhjvdWPmIn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/chest_xray/'\n",
        "\n",
        "train_path = data_path + 'train'\n",
        "valid_path = data_path + 'val'\n",
        "test_path = data_path + 'test'"
      ],
      "metadata": {
        "id": "Cb1qWyRGmJLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "\n",
        "print(f'number of train data: {len(glob(train_path + \"/*/*\"))}')\n",
        "print(f'number of validation data: {len(glob(valid_path + \"/*/*\"))}')\n",
        "print(f'number of test data: {len(glob(test_path + \"/*/*\"))}')"
      ],
      "metadata": {
        "id": "hqAuepanyMYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "os.environ['PYTHONHASHSEED'] = '73'\n",
        "\n",
        "seed = 73\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "img_map = []\n",
        "\n",
        "def prepareData(Dir, start):\n",
        "  category = [\"NORMAL\", \"PNEUMONIA\"]\n",
        "  for category in category:\n",
        "    path = os.path.join(Dir, category)\n",
        "    class_num = category.index(category)\n",
        "\n",
        "    for img in tqdm(os.listdir(path)):\n",
        "      img_path = os.path.join(path, img)\n",
        "      img_map.append({'path':img_path, 'label':category})\n",
        "\n",
        "prepareData(train_path, 'train')\n",
        "prepareData(valid_path, 'val')\n",
        "prepareData(test_path, 'test')\n",
        "\n",
        "img_map = pd.DataFrame(img_map).sample(frac=1, random_state=seed)"
      ],
      "metadata": {
        "id": "EuwSgqtNyjmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_map.shape"
      ],
      "metadata": {
        "id": "KjpfrvdVDsya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "features = img_map['path'].to_numpy()\n",
        "labels = img_map['label'].to_numpy()\n",
        "\n",
        "stratified_sample = StratifiedShuffleSplit(n_splits=2, test_size=0.3, random_state=73)\n"
      ],
      "metadata": {
        "id": "Lt79Jot33lzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for train_index, test_index in stratified_sample.split(features, labels):\n",
        "  X_train, test_X = features[train_index], features[test_index]\n",
        "  y_train, test_y = labels[train_index], labels[test_index]\n",
        "\n",
        "half_size = np.int(len(test_X)/2)\n",
        "X_test, y_test = test_X[0:half_size], test_y[0:half_size]\n",
        "X_val, y_val = test_X[half_size:], test_y[half_size:]"
      ],
      "metadata": {
        "id": "9OUiDtn4EZ_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_map = pd.DataFrame()\n",
        "train_map['path'], train_map['label'] = X_train, y_train\n",
        "\n",
        "test_map = pd.DataFrame()\n",
        "test_map['path'], test_map['label'] = X_test, y_test\n",
        "\n",
        "val_map = pd.DataFrame()\n",
        "val_map['path'], val_map['label'] = X_val, y_val"
      ],
      "metadata": {
        "id": "ishctiiwGfif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data summary\n",
        "print('> {} train size'.format(X_train.shape[0]))\n",
        "print('> {} test size'.format(X_test.shape[0]))\n",
        "print('> {} val size'.format(X_val.shape[0]))"
      ],
      "metadata": {
        "id": "-nkMk8WVGndV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import time\n",
        "import imageio\n",
        "\n",
        "ColorCh = 3\n",
        "IMG_SIZE = 224\n",
        "input_shape=(IMG_SIZE, IMG_SIZE, ColorCh)\n",
        "\n",
        "classes = (\"NORMAL\", \"PNEMONIA\")\n",
        "CATEGORIES = sorted(classes)\n",
        "\n",
        "print('classes:', CATEGORIES)"
      ],
      "metadata": {
        "id": "g-FftpsYGtOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "\n",
        "datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                             horizontal_flip=True,\n",
        "                             brightness_range=[1.0, 1.3],\n",
        "                             rotation_range=15\n",
        "                             )"
      ],
      "metadata": {
        "id": "P7BakqA7HjeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "def get_generator(frame_):\n",
        "  generator = datagen.flow_from_dataframe(\n",
        "      dataframe=frame_,\n",
        "      x_col = 'path',\n",
        "      y_col = 'label',\n",
        "      batch_size=batch_size,\n",
        "      seed=seed,\n",
        "      shuffle = False,\n",
        "      class_mode='sparse',\n",
        "      color_mode='rgb',\n",
        "      save_format='jpeg',\n",
        "      target_size=(IMG_SIZE, IMG_SIZE)\n",
        "  )\n",
        "  return generator"
      ],
      "metadata": {
        "id": "bPtruKamKwAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getLabelCount(frame):\n",
        "    label_count = pd.Series(frame['label'].values.ravel()).value_counts()\n",
        "    n_classes = (label_count)\n",
        "    return label_count"
      ],
      "metadata": {
        "id": "0ZyLMzRUMK_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_map.sample(frac=1, random_state=seed)\n",
        "train_generator = get_generator(train_df)\n",
        "\n",
        "print('훈련 셋의 라벨 갯수')\n",
        "getLabelCount(train_df)"
      ],
      "metadata": {
        "id": "Ryynehp0LbM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df = val_map.sample(frac=1, random_state=seed)\n",
        "val_generator = get_generator(val_df)\n",
        "\n",
        "print('검증 셋의 라벨 갯수')\n",
        "getLabelCount(val_df)"
      ],
      "metadata": {
        "id": "IShaJXULMs5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = test_map.sample(frac=1, random_state=seed)\n",
        "test_generator = get_generator(test_df)\n",
        "\n",
        "print('테스트 셋의 라벨 갯수')\n",
        "getLabelCount(test_df)"
      ],
      "metadata": {
        "id": "0LOiWURmL-5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "g4H7AIxzM4r8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Models"
      ],
      "metadata": {
        "id": "YgWmKpIXM6Kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.backend import separable_conv2d\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Add, add\n",
        "from tensorflow.keras.layers import InputLayer, Input, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Activation, MaxPool2D, ZeroPadding2D, SeparableConv2D\n",
        "from keras.layers.normalization import batch_normalization\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from keras import regularizers\n",
        "\n",
        "kernel_regularizer = regularizers.l2(0.0001)\n",
        "\n",
        "final_activation = 'softmax'\n",
        "entropy = 'sparse_categorical_crossentropy'\n",
        "n_classes = len(CATEGORIES)\n",
        "print(n_classes)"
      ],
      "metadata": {
        "id": "ST_m5__iM2rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FCLayers(baseModel):\n",
        "  baseModel.trainable = True\n",
        "  headModel = baseModel.output\n",
        "  headModel = Dropout(0.5, seed=73)(headModel)\n",
        "  headModel = Dense(n_classes, activtion=final_activation)(headModel)\n",
        "  model = Model(inputs = baseModel.input, outputs = headModel)\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "038TQz5WOXAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GoogLenet 모델"
      ],
      "metadata": {
        "id": "OsM0eSPKPxYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inception Block 만들기\n",
        "\n",
        "from keras.layers.merge import concatenate\n",
        "\n",
        "def Inception_block(input_layer, f1, f2, f3, f4):    \n",
        "    \n",
        "    path1 = Conv2D(filters=f1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n",
        "    \n",
        "    path2 = Conv2D(filters = f2[0], kernel_size = (1,1), \n",
        "                   padding = 'same', activation = 'relu')(input_layer)\n",
        "    \n",
        "    path2 = Conv2D(filters = f2[1], kernel_size = (3,3), \n",
        "                   padding = 'same', activation = 'relu')(path2)\n",
        "\n",
        "    path3 = Conv2D(filters = f3[0], kernel_size = (1,1), \n",
        "                   padding = 'same', activation = 'relu')(input_layer)\n",
        "    \n",
        "    path3 = Conv2D(filters = f3[1], kernel_size = (5,5), \n",
        "                   padding = 'same', activation = 'relu')(path3)\n",
        "\n",
        "    path4 = MaxPooling2D((3,3), strides= (1,1), \n",
        "                         padding = 'same')(input_layer)\n",
        "    \n",
        "    path4 = Conv2D(filters = f4, kernel_size = (1,1), \n",
        "                   padding = 'same', activation = 'relu')(path4)\n",
        "    \n",
        "    output_layer = concatenate([path1, path2, path3, path4], axis = -1)\n",
        "\n",
        "    return output_layer"
      ],
      "metadata": {
        "id": "7tEhyzXVPw-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Extra_network_2(X):\n",
        "  X2 = AveragePooling2D(pool_size = (5,5), strides = 3)(X)\n",
        "  X2 = Conv2D(filters =128, kernel_size = (1,1), padding = 'same', activation = 'relu')(X2)\n",
        "\n",
        "  X2 = Flatten()(X2)\n",
        "  X2 = Dense(1024, activation=final_activation, name = 'output2')(X2)\n",
        "  return X2\n",
        "\n",
        "def Extra_network_1(X):\n",
        "  X1 = AveragePooling2D(pool_size = (5, 5), strides =3)(X)\n",
        "  X1 = Conv2D(filters = 128, kernel_size = (1, 1), padding = 'same', activation = 'relu')(X1)\n",
        "\n",
        "  X1 = Flatten()(X1)\n",
        "  X1 = Dense(1024, activation = 'relu')(X1)\n",
        "  X1 = Dense(n_classes, activation = final_activation, name='output1')(X1)\n",
        "  return X1"
      ],
      "metadata": {
        "id": "WClEGaNiPvWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_4(X):\n",
        "  X = Inception_block(X, 192, (96, 208), (16, 48), 128)\n",
        "\n",
        "  X1 = Extra_network_1(X)\n",
        "\n",
        "  X = Inception_block(X, 160, (112, 224), (24, 64), 64)\n",
        "  X = Inception_block(X, 128, (128, 256), (24, 64), 64)\n",
        "  X = Inception_block(X, 112, (144, 288), (32, 64), 64)\n",
        "\n",
        "  X2 = Extra_network_2(X)\n",
        "\n",
        "  X = Inception_block(X, 256, (160, 320), (32, 128), 128)\n",
        "  X = MaxPooling2D(pool_size = 3, strides = 2)(X)\n",
        "\n",
        "  return X, X1, X2\n",
        "\n",
        "def layer_3(X):\n",
        "  X = Inception_block(X, 64, (96, 128), (16, 32), 32)\n",
        "  X = Inception_block(X, 128, (128, 192), (32, 96), 64)\n",
        "  X = MaxPooling2D(pool_size=(3,3), strides=2)(X)\n",
        "\n",
        "  return X\n",
        "\n",
        "def layer_2(X):\n",
        "  X = Conv2D(filters=64, kernel_size=1, strides = 1, padding = 'same', activation = 'relu')(X)\n",
        "  X = Conv2D(filters = 192, kernel_size = 3, padding = 'same', activation='relu')(X)\n",
        "  X = MaxPooling2D(pool_size=3, strides=2)(X)\n",
        "\n",
        "  return X"
      ],
      "metadata": {
        "id": "J07BDV3ExH7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_GoogLeNet():\n",
        "  input_layer = Input(shape=input_shape)\n",
        "\n",
        "  X = Conv2D(64, kernel_size=7, strides=2, padding='valid', activation='relu')(input_layer)\n",
        "\n",
        "  X = MaxPooling2D(pool_size =3, strides=2)(X)\n",
        "\n",
        "  X = layer_2(X)\n",
        "  X = layer_3(X)\n",
        "  X, X1, X2 = layer_4(X)\n",
        "\n",
        "  X = Inception_block(X, 256, (160, 320), (32, 128), 128)\n",
        "  X = Inception_block(X, 384, (192, 384), (48, 128), 128)\n",
        "\n",
        "  X = GlobalAveragePooling2D()(X)\n",
        "  X = Dropout(0.6)(X)\n",
        "\n",
        "  X = Dense(n_classes, activation=final_activation, name='output3')(X)\n",
        "\n",
        "  model = Model(input_layer, [X, X1, X2], name = 'GoogLeNet')\n",
        "\n",
        "  return model\n",
        "\n",
        "load_GoogLeNet().summary()"
      ],
      "metadata": {
        "id": "Ed98ot20yd8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping, ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "DGmgRVXe8Y2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EPOCHS = 1\n",
        "EPOCHS = 120\n",
        "patience = 3\n",
        "\n",
        "start_lr = 0.00001\n",
        "min_lr = 0.00001\n",
        "max_lr = 0.00005\n",
        "\n",
        "rampup_epochs = 5\n",
        "sustain_epochs = 0\n",
        "exp_decay = .8\n",
        "        \n",
        "def lrfn(epoch):\n",
        "    if epoch < rampup_epochs:\n",
        "        return (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n",
        "    elif epoch < rampup_epochs + sustain_epochs:\n",
        "        return max_lr\n",
        "    else:\n",
        "        return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n",
        "   \n",
        "def getCallbacks(name):\n",
        "    DESIRED_ACCURACY = 0.90\n",
        "    class myCallback(tf.keras.callbacks.Callback):\n",
        "        def on_epoch_end(self, epoch, logs={}):\n",
        "            if (logs.get('accuracy')is not None and logs.get('accuracy') >= DESIRED_ACCURACY):\n",
        "                print(\"\\nLimits Reached cancelling training!\")\n",
        "                self.model.stop_training = True\n",
        "\n",
        "            \n",
        "    end_callback = myCallback()\n",
        "\n",
        "    lr_plat = ReduceLROnPlateau(patience = 2, mode = 'min')\n",
        "\n",
        "    lr_callback = LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=False)\n",
        "\n",
        "    early_stopping = EarlyStopping(patience = patience, monitor='val_loss',\n",
        "                                 mode='min', restore_best_weights=True, \n",
        "                                 verbose = 1, min_delta = .00075)\n",
        "\n",
        "\n",
        "    checkpoint_filepath = name + '_Weights.h5'\n",
        "\n",
        "    model_checkpoints = ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                        save_weights_only=True,\n",
        "                                        monitor='val_loss',\n",
        "                                        mode='min',\n",
        "                                        verbose = 1,\n",
        "                                        save_best_only=True)\n",
        "\n",
        "    import datetime\n",
        "    log_dir=\"logs/fit/\" + '_' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")  \n",
        "    tensorboard_callback = TensorBoard(log_dir = log_dir, write_graph=True, histogram_freq=1)\n",
        "\n",
        "    return [end_callback, \n",
        "             lr_callback, \n",
        "             model_checkpoints,\n",
        "             early_stopping,\n",
        "             #tensorboard_callback,\n",
        "             lr_plat\n",
        "            ]\n",
        "\n",
        "GoogLeNet_callbacks = getCallbacks('GoogLeNet')"
      ],
      "metadata": {
        "id": "m4Hefxl-33y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CompileModel(name, model):\n",
        "    if name == 'GoogLeNet':\n",
        "        model.compile(optimizer='adam', loss=entropy, metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def FitModel(model, name):\n",
        "    if name == 'GoogLeNet':\n",
        "        callbacks_ = GoogLeNet_callbacks\n",
        "    history = model.fit(train_generator, \n",
        "                        epochs=EPOCHS,\n",
        "                        callbacks=callbacks_,\n",
        "                        validation_data = val_generator,\n",
        "                        steps_per_epoch=(len(train_generator.labels) / 80),\n",
        "                        validation_steps=(len(val_generator.labels) / 80),\n",
        "                       )\n",
        "    \n",
        "    print(history.history.keys())\n",
        "\n",
        "    model.load_weights(name + '_Weights.h5')\n",
        "\n",
        "    final_accuracy_avg = np.mean(history.history['val_output3_accuracy'][-4:])\n",
        "\n",
        "    final_loss = history.history['val_loss'][-1]\n",
        "  \n",
        "    group = {(history): 'history', (model): 'model', (final_accuracy_avg):'acc', (final_loss): 'loss'}\n",
        "\n",
        "    print('\\n')\n",
        "    print('---'*15)\n",
        "    print(name,' Model')\n",
        "    print('Total Epochs :', len(history.history['loss']))    \n",
        "    print('Restoring best Weights')\n",
        "    \n",
        "    index = (len(history.history['loss']) - (patience + 1))\n",
        "    print('---'*15)\n",
        "    print('Best Epoch :', index)\n",
        "    print('---'*15)\n",
        "    \n",
        "    train_accuracy = history.history['output3_accuracy']\n",
        "    train_loss = history.history['loss']\n",
        "    \n",
        "    val_accuracy = history.history['val_output3_accuracy']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    print('Accuracy on train:', train_accuracy,\n",
        "          '\\tLoss on train:', train_loss)\n",
        "    \n",
        "    print('Accuracy on val:', val_accuracy ,\n",
        "          '\\tLoss on val:', val_loss)\n",
        "    print('---'*15)\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "qtlLhBTf3TIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def BuildModel(name):\n",
        "    if name == 'GoogLeNet':\n",
        "        prepared_model = load_GoogLeNet() \n",
        "        \n",
        "    compiled_model = CompileModel(name, prepared_model)\n",
        "    return compiled_model"
      ],
      "metadata": {
        "id": "PBN5O_bG3osD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_compiled_model = BuildModel('GoogLeNet')\n",
        "g_model, g_history = FitModel(g_compiled_model, 'GoogLeNet')"
      ],
      "metadata": {
        "id": "Lk3X_Zb_3rY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation on the Testset"
      ],
      "metadata": {
        "id": "SsvebnGsTEqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "def print_graph(item, index, history):\n",
        "    plt.figure()\n",
        "    train_values = history.history[item][0:index]\n",
        "    plt.plot(train_values)\n",
        "    test_values = history.history['val_' + item][0:index]\n",
        "    plt.plot(test_values)\n",
        "    plt.legend(['training','validation'])\n",
        "    plt.title('Training and validation '+ item)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.show()\n",
        "    plot = '{}.png'.format(item)\n",
        "    plt.savefig(plot)"
      ],
      "metadata": {
        "id": "K_MJpVumQ5Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, plot_roc_curve, accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "def test_set_results(pred_value, n=1):    \n",
        "    y_test = test_generator.labels\n",
        "    X_test, _ = test_generator.next()\n",
        "    \n",
        "    corr_pred = metrics.confusion_matrix(y_test, pred_value)\n",
        "    fig=plt.figure(figsize=(10, 8))\n",
        "    ax = plt.axes()\n",
        "    \n",
        "    sns.heatmap(corr_pred,annot=True, fmt=\"d\",cmap=\"Purples\", xticklabels=CATEGORIES, yticklabels=CATEGORIES)\n",
        "    ax.set_title('Dense Output {}'.format(n))\n",
        "    plt.show()\n",
        "    \n",
        "    n_correct = np.int(corr_pred[0][0] + corr_pred[1][1])\n",
        "    print('...'*15)\n",
        "\n",
        "    print('> Correct Predictions:', n_correct)\n",
        "    \n",
        "    n_wrongs = len(y_test) - n_correct\n",
        "    print('> Wrong Predictions:', n_wrongs)\n",
        "    print('...'*15)\n",
        "    \n",
        "    print(classification_report(test_generator.labels, pred_value, target_names=CATEGORIES))"
      ],
      "metadata": {
        "id": "hGJ2zMvMfgB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printResults(name, model):\n",
        "    predictions = model.predict(test_generator, verbose=1)\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "    test_set_results(preds)"
      ],
      "metadata": {
        "id": "w27T8AEVfiWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_summary(model, history, name):\n",
        "    index = (len(history.history['loss']) - (patience + 1))\n",
        "    print('Best Epochs: ', index)\n",
        "    \n",
        "    if name == 'GoogLeNet':\n",
        "        results = model.evaluate(test_generator, verbose=1)\n",
        "        loss, output3_loss, output1_loss, output2_loss, output3_accuracy, output1_accuracy, output2_accuracy = results\n",
        "        \n",
        "        for i in range(3):\n",
        "            n = i + 1\n",
        "            out_layer = 'Output Layer {}'.format(n)\n",
        "            \n",
        "            if n == 1:\n",
        "                test_accuracy = output1_accuracy\n",
        "                test_loss = output1_loss\n",
        "\n",
        "            if n == 2:\n",
        "                test_accuracy = output2_accuracy\n",
        "                test_loss = output2_loss\n",
        "                \n",
        "            if n == 3:\n",
        "                test_accuracy = output3_accuracy\n",
        "                test_loss = output3_loss\n",
        "                \n",
        "                \n",
        "            output_name = 'output{}_'.format(n)\n",
        "            train_accuracy, train_loss = history.history[output_name + 'accuracy'][index], history.history[output_name + 'loss'][index]\n",
        "            \n",
        "  \n",
        "            print_graph(output_name + 'loss', index, history)\n",
        "            print_graph(output_name + 'accuracy', index, history)\n",
        "        \n",
        "            print('---'*15)  \n",
        "            print('GoogLeNet Dense output {}:'.format(n))\n",
        "            \n",
        "            print('> Accuracy on train :'.format(out_layer), train_accuracy, \n",
        "                  '\\tLoss on train:',train_loss)\n",
        "        \n",
        "            print('> Accuracy on test :'.format(out_layer), test_accuracy,\n",
        "                  '\\tLoss on test:',test_loss)\n",
        "            \n",
        "            print('---'*15)\n",
        "            print('> predicting test')\n",
        "            print('---'*15)\n",
        "            \n",
        "            predictions = model.predict(test_generator, verbose=1)\n",
        "            preds = np.argmax(predictions[i], axis=1)\n",
        "            test_set_results(preds, n)\n",
        "                \n",
        "    else:\n",
        "        test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)\n",
        "        \n",
        "        train_accuracy = history.history['accuracy'][index]\n",
        "        train_loss = history.history['loss'][index]\n",
        "\n",
        "        print_graph('loss', index, history)\n",
        "        print_graph('accuracy', index, history)\n",
        "        \n",
        "        print('---'*15) \n",
        "        print(name)\n",
        "        print('> Accuracy on train:',train_accuracy, \n",
        "              '\\tLoss on train:',train_loss)\n",
        "        \n",
        "        print('> Accuracy on test:',test_accuracy,\n",
        "              '\\tLoss on test:',test_loss)\n",
        "        \n",
        "        print('---'*15)\n",
        "        print('> predicting test')\n",
        "        print('---'*15)\n",
        "        \n",
        "        printResults(name, model)"
      ],
      "metadata": {
        "id": "cibaUliBTY_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printResults(name, model):\n",
        "    predictions = model.predict(test_generator, verbose=1)\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "    test_set_results(preds)"
      ],
      "metadata": {
        "id": "KXDGLev3Tc2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_summary(model, history, name):\n",
        "    index = (len(history.history['loss']) - (patience + 1))\n",
        "    print('Best Epochs: ', index)\n",
        "    \n",
        "    if name == 'GoogLeNet':\n",
        "        results = model.evaluate(test_generator, verbose=1)\n",
        "        loss, output3_loss, output1_loss, output2_loss, output3_accuracy, output1_accuracy, output2_accuracy = results\n",
        "        \n",
        "        for i in range(3):\n",
        "            n = i + 1\n",
        "            out_layer = 'Output Layer {}'.format(n)\n",
        "            \n",
        "            if n == 1:\n",
        "                test_accuracy = output1_accuracy\n",
        "                test_loss = output1_loss\n",
        "\n",
        "            if n == 2:\n",
        "                test_accuracy = output2_accuracy\n",
        "                test_loss = output2_loss\n",
        "                \n",
        "            if n == 3:\n",
        "                test_accuracy = output3_accuracy\n",
        "                test_loss = output3_loss\n",
        "                \n",
        "                \n",
        "            output_name = 'output{}_'.format(n)\n",
        "            train_accuracy, train_loss = history.history[output_name + 'accuracy'][index], history.history[output_name + 'loss'][index]\n",
        "            \n",
        "  \n",
        "            print_graph(output_name + 'loss', index, history)\n",
        "            print_graph(output_name + 'accuracy', index, history)\n",
        "        \n",
        "            print('---'*15)  \n",
        "            print('GoogLeNet Dense output {}:'.format(n))\n",
        "            \n",
        "            print('> Accuracy on train :'.format(out_layer), train_accuracy, \n",
        "                  '\\tLoss on train:',train_loss)\n",
        "        \n",
        "            print('> Accuracy on test :'.format(out_layer), test_accuracy,\n",
        "                  '\\tLoss on test:',test_loss)\n",
        "            \n",
        "            print('---'*15)\n",
        "            print('> predicting test')\n",
        "            print('---'*15)\n",
        "            \n",
        "            predictions = model.predict(test_generator, verbose=1)\n",
        "            preds = np.argmax(predictions[i], axis=1)\n",
        "            test_set_results(preds, n)\n",
        "                \n",
        "    else:\n",
        "        test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)\n",
        "        \n",
        "        train_accuracy = history.history['accuracy'][index]\n",
        "        train_loss = history.history['loss'][index]\n",
        "\n",
        "        print_graph('loss', index, history)\n",
        "        print_graph('accuracy', index, history)\n",
        "        \n",
        "        print('---'*15) \n",
        "        print(name)\n",
        "        print('> Accuracy on train:',train_accuracy, \n",
        "              '\\tLoss on train:',train_loss)\n",
        "        \n",
        "        print('> Accuracy on test:',test_accuracy,\n",
        "              '\\tLoss on test:',test_loss)\n",
        "        \n",
        "        print('---'*15)\n",
        "        print('> predicting test')\n",
        "        print('---'*15)\n",
        "        \n",
        "        printResults(name, model)"
      ],
      "metadata": {
        "id": "JIz43XF4TghD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_summary(g_model, g_history, 'GoogLeNet')"
      ],
      "metadata": {
        "id": "t2jJ-S8aTLhv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}